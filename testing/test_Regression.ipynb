{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.testing import assert_array_almost_equal\n",
    "import GPy\n",
    "import gp_grief\n",
    "import gp_grief.grid\n",
    "from pdb import set_trace\n",
    "from time import time\n",
    "gp_grief.debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rastrigin(x,lin_term=None):\n",
    "    \"\"\"\n",
    "    Rastrigin test function\n",
    "    input should be in range [-5.12, 5.12]\n",
    "    if x in range [0,1], can transform by rastrigin((x*2-1)*5.12)\n",
    "    if lin_term is not None then will add a linear term to the first dimension. This helps\n",
    "    to make the function non-symetric wrt the input dimensions\n",
    "    \"\"\"\n",
    "    assert x.ndim == 2\n",
    "    d = x.shape[1]\n",
    "    f = 10*d\n",
    "    for i in range(d):\n",
    "        f = f+(np.power(x[:,i,None],2) - 10*np.cos(2*np.pi*x[:,i,None]));\n",
    "    if lin_term is not None:\n",
    "        f += lin_term*x[:,(0,)]\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `gp_grief.models.GPRegression` Testing\n",
    "Will compare with GPy across various problems.\n",
    "\n",
    "Note that this assumes the model defaults are the same as those for GPy.\n",
    "\n",
    "## Sweep accross various Problem Dimensions \n",
    "And consider non-default active dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "d = 1, active_dims = None\n",
      "[ 14:34:00 ] GP INFO: initializing Y\n",
      "[ 14:34:00 ] GP INFO: initializing inference method\n",
      "[ 14:34:00 ] GP INFO: adding kernel and likelihood as parameters\n",
      "[ 14:34:00 ] gp_grief.kern DEBUG: Initializing RBF kernel.\n",
      "[ 14:34:00 ] gp_grief.models DEBUG: Initializing GPRegression model.\n",
      "checking gradient\n",
      "[ 14:34:00 ] gp_grief.models INFO: Gradient check passed.\n",
      "checking initial log-likelihood\n",
      "testing if alpha matches\n",
      "[ 14:34:00 ] gp_grief.models DEBUG: Fitting; determining weight vector.\n",
      "testing if predictions match...\n",
      "[ 14:34:00 ] gp_grief.models DEBUG: Predicting model at new points.\n",
      "... mean\n",
      "... variance\n",
      "checking optimization\n",
      "[ 14:34:00 ] gp_grief.models DEBUG: Beginning MLE to optimize hyperparameters. grad_method=finite_difference chol\n",
      "[ 14:34:01 ] gp_grief.models INFO: Function Evals: 37. Exit status: Converged\n",
      "\n",
      "********************************************************************************\n",
      "d = 10, active_dims = None\n",
      "[ 14:34:01 ] GP INFO: initializing Y\n",
      "[ 14:34:01 ] GP INFO: initializing inference method\n",
      "[ 14:34:01 ] GP INFO: adding kernel and likelihood as parameters\n",
      "[ 14:34:01 ] gp_grief.kern DEBUG: Initializing RBF kernel.\n",
      "[ 14:34:01 ] gp_grief.models DEBUG: Initializing GPRegression model.\n",
      "checking gradient\n",
      "[ 14:34:01 ] gp_grief.models INFO: Gradient check passed.\n",
      "checking initial log-likelihood\n",
      "testing if alpha matches\n",
      "[ 14:34:01 ] gp_grief.models DEBUG: Fitting; determining weight vector.\n",
      "testing if predictions match...\n",
      "[ 14:34:01 ] gp_grief.models DEBUG: Predicting model at new points.\n",
      "... mean\n",
      "... variance\n",
      "checking optimization\n",
      "[ 14:34:01 ] gp_grief.models DEBUG: Beginning MLE to optimize hyperparameters. grad_method=finite_difference chol\n",
      "[ 14:34:02 ] gp_grief.models INFO: Function Evals: 121. Exit status: Converged\n",
      "done tests.\n"
     ]
    }
   ],
   "source": [
    "for d in [1,10]:\n",
    "    for active_dims in [None,]:#, np.int32(np.floor(0.5*d))]:\n",
    "        print '\\n' + '*' * 80\n",
    "        print 'd = %d, active_dims = %s' % (d, repr(active_dims))\n",
    "\n",
    "        # generate data\n",
    "        np.random.seed(0)\n",
    "        N = 100\n",
    "        x = np.random.uniform(size=(N,d)) # generate dataset\n",
    "        y = rastrigin((x*2-1)*5.12, lin_term=1.)\n",
    "        xx = np.random.uniform(size=(N+1,d)) # generate test set\n",
    "\n",
    "        # initialize GPy.models.GPRegression\n",
    "        if active_dims is None:\n",
    "            kern = GPy.kern.RBF(d)\n",
    "            mg = GPy.models.GPRegression(x,y,kern)\n",
    "        else: # I want to consider only a subset of the dimensions\n",
    "            kern = GPy.kern.RBF(np.size(active_dims))\n",
    "            x_reduced = x[:,active_dims].reshape((-1,np.size(active_dims)))\n",
    "            mg = GPy.models.GPRegression(x_reduced, y, kern)\n",
    "        \n",
    "        # initialize gp_grief model\n",
    "        kern = gp_grief.kern.RBF(d, active_dims=active_dims)\n",
    "        m = gp_grief.models.GPRegression(x,y,kern)\n",
    "        \n",
    "        print 'checking gradient'\n",
    "        m.checkgrad() \n",
    "        \n",
    "        print 'checking initial log-likelihood'\n",
    "        assert_array_almost_equal(mg.log_likelihood(), m.log_likelihood(), decimal=3) \n",
    "        \n",
    "        print 'testing if alpha matches'\n",
    "        m.fit()\n",
    "        alpha_gpy = mg.posterior.woodbury_vector\n",
    "        assert_array_almost_equal(alpha_gpy, m._alpha, decimal=3)\n",
    "        \n",
    "        print 'testing if predictions match...'\n",
    "        yyh = m.predict(xx, compute_var='full')\n",
    "        yyh_gpy = mg.predict(xx,full_cov=True)\n",
    "        print '... mean'\n",
    "        assert_array_almost_equal(yyh_gpy[0], yyh[0], decimal=3)\n",
    "        print '... variance'\n",
    "        assert_array_almost_equal(yyh_gpy[1], yyh[1], decimal=2)\n",
    "        \n",
    "        print 'checking optimization'\n",
    "        m.optimize()\n",
    "        #mg.optimize()\n",
    "        #ll_gpy_opt = mg.log_likelihood()\n",
    "        #assert_array_almost_equal(ll_gpy_opt, m.log_likelihood(), decimal=2) \n",
    "print 'done tests.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining kernels\n",
    "Try to reconstruct the kernel\n",
    "$$ k = (k_0 k_1 + k_2)k_3$$\n",
    "\n",
    "(see `kernel.testing.ipynb` for covariance matrix testing of this same kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 14:34:02 ] gp_grief.kern DEBUG: Initializing k0 kernel.\n",
      "[ 14:34:02 ] gp_grief.kern DEBUG: Initializing k1 kernel.\n",
      "[ 14:34:02 ] gp_grief.kern DEBUG: Initializing k2 kernel.\n",
      "[ 14:34:02 ] gp_grief.kern DEBUG: Initializing k3 kernel.\n",
      "[ 14:34:02 ] GP INFO: initializing Y\n",
      "[ 14:34:02 ] GP INFO: initializing inference method\n",
      "[ 14:34:02 ] GP INFO: adding kernel and likelihood as parameters\n",
      "[ 14:34:02 ] gp_grief.models DEBUG: Initializing GPRegression model.\n",
      "\n",
      "GPRegression Model\n",
      "| Name      |   Value | Constraint   |\n",
      "|-----------+---------+--------------|\n",
      "| noise_var |       1 | +ve          |\n",
      "\n",
      "k0 kernel\n",
      "| Name        |   Value | Constraint   |\n",
      "|-------------+---------+--------------|\n",
      "| variance    |     0.5 | +ve          |\n",
      "| lengthscale |     0.5 | +ve          |\n",
      "\n",
      "mul with child:\n",
      "\n",
      "k1 kernel\n",
      "| Name        |   Value | Constraint   |\n",
      "|-------------+---------+--------------|\n",
      "| variance    |       1 | fixed        |\n",
      "| lengthscale |       1 | +ve          |\n",
      "\n",
      "\n",
      "add with child:\n",
      "\n",
      "k2 kernel\n",
      "| Name        |   Value | Constraint   |\n",
      "|-------------+---------+--------------|\n",
      "| variance    |     1.5 | +ve          |\n",
      "| lengthscale |     1.5 | +ve          |\n",
      "\n",
      "\n",
      "mul with child:\n",
      "\n",
      "k3 kernel\n",
      "| Name        |   Value | Constraint   |\n",
      "|-------------+---------+--------------|\n",
      "| variance    |       2 | fixed        |\n",
      "| lengthscale |       2 | +ve          |\n",
      "\n",
      "\n",
      "\n",
      "Name : GP regression\n",
      "Objective : 18637.1667801\n",
      "Number of Parameters : 9\n",
      "Number of Optimization Parameters : 7\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mGP_regression.            \u001b[0;0m  |  value  |  constraints  |  priors\n",
      "  \u001b[1mmul.sum.mul.k0.variance   \u001b[0;0m  |    0.5  |      +ve      |        \n",
      "  \u001b[1mmul.sum.mul.k0.lengthscale\u001b[0;0m  |    0.5  |      +ve      |        \n",
      "  \u001b[1mmul.sum.mul.k1.variance   \u001b[0;0m  |    1.0  |   +ve fixed   |        \n",
      "  \u001b[1mmul.sum.mul.k1.lengthscale\u001b[0;0m  |    1.0  |      +ve      |        \n",
      "  \u001b[1mmul.sum.k2.variance       \u001b[0;0m  |    1.5  |      +ve      |        \n",
      "  \u001b[1mmul.sum.k2.lengthscale    \u001b[0;0m  |    1.5  |      +ve      |        \n",
      "  \u001b[1mmul.k3.variance           \u001b[0;0m  |    2.0  |   +ve fixed   |        \n",
      "  \u001b[1mmul.k3.lengthscale        \u001b[0;0m  |    2.0  |      +ve      |        \n",
      "  \u001b[1mGaussian_noise.variance   \u001b[0;0m  |    1.0  |      +ve      |        \n",
      "checking gradient\n",
      "[ 14:34:02 ] gp_grief.models INFO: Gradient check passed.\n",
      "checking initial log-likelihood\n",
      "testing if predictions match...\n",
      "[ 14:34:02 ] gp_grief.models DEBUG: Predicting model at new points.\n",
      "[ 14:34:02 ] gp_grief.models DEBUG: Fitting; determining weight vector.\n",
      "... mean\n",
      "... variance\n",
      "testing if alpha matches\n",
      "done tests.\n",
      "checking final log-likelihood after optimizing\n",
      "[ 14:34:02 ] gp_grief.models DEBUG: Beginning MLE to optimize hyperparameters. grad_method=finite_difference chol\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(4.3128709631594675e-23)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(7.241626554942036e-23)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(4.3128709631594675e-23)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(7.241626554942036e-23)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(4.3128709631594675e-23)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(7.241626554942036e-23)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(4.3128709631594675e-23)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(7.241626554942036e-23)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(4.3128709631594675e-23)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(7.241626554942036e-23)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(7.241626554942036e-23)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(4.3128709631594675e-23)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(4.3128709631594675e-23)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(7.241626554942036e-23)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1e-200)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1e-200)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1e-200)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1e-200)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1e-200)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1e-200)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1e-200)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1e-200)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1e-200)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1e-200)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1e-200)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1e-200)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1e-200)).\n",
      "[ 14:34:10 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1e-200)).\n",
      "[ 14:34:11 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(4.918340853950816e-33)).\n",
      "[ 14:34:11 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1.978294794854775e-29)).\n",
      "[ 14:34:11 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(4.918340853950816e-33)).\n",
      "[ 14:34:11 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1.978294794854775e-29)).\n",
      "[ 14:34:11 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(4.918340853950816e-33)).\n",
      "[ 14:34:11 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1.978294794854775e-29)).\n",
      "[ 14:34:11 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(4.918340853950816e-33)).\n",
      "[ 14:34:11 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1.978294794854775e-29)).\n",
      "[ 14:34:11 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(4.918340853950816e-33)).\n",
      "[ 14:34:11 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1.978294794854775e-29)).\n",
      "[ 14:34:11 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1.978294794854775e-29)).\n",
      "[ 14:34:11 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(4.918340853950816e-33)).\n",
      "[ 14:34:11 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(4.918340853950816e-33)).\n",
      "[ 14:34:11 ] gp_grief.kern DEBUG: protected RBF against zero-division since lengthscale too small (array(1.978294794854775e-29)).\n",
      "[ 14:34:14 ] gp_grief.models INFO: Function Evals: 128. Exit status: Converged\n",
      "done tests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/trefor/Documents/libraries/GPy/GPy/kern/src/stationary.py:160: RuntimeWarning:overflow encountered in divide\n",
      " /home/trefor/Documents/libraries/GPy/GPy/kern/src/rbf.py:35: RuntimeWarning:overflow encountered in square\n",
      " /home/trefor/Documents/libraries/GPy/GPy/kern/src/rbf.py:38: RuntimeWarning:invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "# generate data\n",
    "np.random.seed(1)\n",
    "N = 100\n",
    "d = 5\n",
    "x = np.random.uniform(size=(N,d)) # generate dataset\n",
    "y = rastrigin((x*2-1)*5.12, lin_term=1.)\n",
    "xx = np.random.uniform(size=(N+1,d)) # generate test set\n",
    "    \n",
    "# first define the base kernels\n",
    "kb_gpy = [GPy.kern.RBF(    d, lengthscale=0.5*i+0.5, variance=0.5*i+0.5, name='k%d' % i) \n",
    "          for i in range(4)]\n",
    "kb_kml = [gp_grief.kern.RBF(d, lengthscale=0.5*i+0.5, variance=0.5*i+0.5, name='k%d' % i) \n",
    "          for i in range(4)]\n",
    "\n",
    "# then combine the base kernels\n",
    "k_gpy = ((kb_gpy[0] * kb_gpy[1]) + kb_gpy[2]) * kb_gpy[3]\n",
    "k_kml = ((kb_kml[0] * kb_kml[1]) + kb_kml[2]) * kb_kml[3]\n",
    "\n",
    "# check to ensure they give the same kernel covariance matrix (also done in kernel_testing)\n",
    "# ... this just ensures that everything was set up correctly\n",
    "assert_array_almost_equal( k_kml.cov(x), k_gpy.K(x) )\n",
    "\n",
    "# construct the models\n",
    "m = dict()\n",
    "m['gpy'] = GPy.models.GPRegression(  x,y,k_gpy)\n",
    "m['kml'] = gp_grief.models.GPRegression(x,y,k_kml)\n",
    "m['gpy'].mul.sum.mul.k1.variance.fix()\n",
    "m['gpy'].mul.k3.variance.fix()\n",
    "for key in m:\n",
    "    print m[key]\n",
    "\n",
    "print 'checking gradient'\n",
    "assert m['kml'].checkgrad()\n",
    "\n",
    "print 'checking initial log-likelihood'\n",
    "assert_array_almost_equal(m['gpy'].log_likelihood(),m['kml'].log_likelihood(), decimal=3) \n",
    "\n",
    "print 'testing if predictions match...'\n",
    "yyh = dict()\n",
    "yyh['gpy'] = m['gpy'].predict(xx, full_cov=True)\n",
    "yyh['kml'] = m['kml'].predict(xx, compute_var='full')\n",
    "print '... mean'\n",
    "assert_array_almost_equal(*[yyh[key][0] for key in m], decimal=2) \n",
    "print '... variance'\n",
    "assert_array_almost_equal(*[yyh[key][1] for key in m], decimal=2) \n",
    "\n",
    "print 'testing if alpha matches'\n",
    "assert_array_almost_equal(m['gpy'].posterior.woodbury_vector, m['kml']._alpha, decimal=3)\n",
    "print 'done tests.'\n",
    "\n",
    "print 'checking final log-likelihood after optimizing'\n",
    "for key in m:\n",
    "    m[key].optimize()\n",
    "assert_array_almost_equal(m['gpy'].log_likelihood(),m['kml'].log_likelihood(), decimal=-1) \n",
    "print 'done tests.'"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
